{
 "cells": [
  {
   "cell_type": "code",
   "source": "# DATA PERSISTENCE: Load from previous notebooks\nimport sys\nsys.path.append('.')\n\ntry:\n    from notebook_bridge import load_notebook_data, load_latest_data\n    \n    print(\"üîó LOADING DATA FROM PREVIOUS NOTEBOOKS...\")\n    \n    # Try to load from notebook 2 (preprocessing) first\n    nb2_data = load_notebook_data(2)\n    if nb2_data:\n        print(\"‚úÖ Found preprocessed data from Notebook 2\")\n        \n        # Load key variables\n        if 'df' in nb2_data:\n            df = nb2_data['df']\n        if 'preprocessor' in nb2_data:\n            preprocessor = nb2_data['preprocessor'] \n        if 'X_train' in nb2_data and 'X_test' in nb2_data:\n            # Load train/test data\n            train_df = pd.read_csv('../data/persistence/notebook2_train_data.csv')\n            test_df = pd.read_csv('../data/persistence/notebook2_test_data.csv')\n            X_train = train_df['text']\n            y_train = train_df['label']\n            X_test = test_df['text'] \n            y_test = test_df['label']\n            print(f\"üìä Loaded train/test splits: {len(X_train)} train, {len(X_test)} test\")\n        \n        # Load baseline results for comparison\n        if 'results' in nb2_data:\n            baseline_results = nb2_data['results']\n            print(f\"üìà Baseline best accuracy: {baseline_results.get('best_accuracy', 'N/A')}\")\n    \n    else:\n        # Fallback to loading raw data\n        print(\"‚ö†Ô∏è No preprocessed data found, will need to preprocess...\")\n        needs_preprocessing = True\n        \nexcept ImportError:\n    print(\"‚ö†Ô∏è notebook_bridge not available, using manual loading...\")\n    \n    # Manual loading as fallback\n    try:\n        df = pd.read_csv('../data/persistence/notebook2_processed_df.csv')\n        train_df = pd.read_csv('../data/persistence/notebook2_train_data.csv')\n        test_df = pd.read_csv('../data/persistence/notebook2_test_data.csv')\n        \n        X_train = train_df['text']\n        y_train = train_df['label'] \n        X_test = test_df['text']\n        y_test = test_df['label']\n        \n        import pickle\n        with open('../data/persistence/notebook2_preprocessor.pkl', 'rb') as f:\n            preprocessor = pickle.load(f)\n            \n        print(\"‚úÖ Manually loaded data from Notebook 2\")\n        print(f\"üìä Dataset: {df.shape}, Train: {len(X_train)}, Test: {len(X_test)}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Error loading previous data: {e}\")\n        print(\"Will load from raw data and preprocess...\")\n        needs_preprocessing = True\n\nprint(\"üöÄ Ready to start CNN modeling!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Text Classification Model\n",
    "\n",
    "This notebook implements a Convolutional Neural Network for fake news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'TextPreprocessor' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m df = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../data/combined_news_dataset.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load the preprocessor\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m preprocessor = \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m../models/text_preprocessor.pkl\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLabel distribution:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mdf[\u001b[33m'\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m'\u001b[39m].value_counts()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fake-news-classification/fake_news_env/lib/python3.13/site-packages/joblib/numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/fake-news-classification/fake_news_env/lib/python3.13/site-packages/joblib/numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1256\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1254\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1256\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1257\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1258\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1581\u001b[39m, in \u001b[36m_Unpickler.load_stack_global\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1579\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(module) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m   1580\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\u001b[33m\"\u001b[39m\u001b[33mSTACK_GLOBAL requires str\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1581\u001b[39m \u001b[38;5;28mself\u001b[39m.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfind_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:1624\u001b[39m, in \u001b[36m_Unpickler.find_class\u001b[39m\u001b[34m(self, module, name)\u001b[39m\n\u001b[32m   1622\u001b[39m \u001b[38;5;28m__import__\u001b[39m(module, level=\u001b[32m0\u001b[39m)\n\u001b[32m   1623\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.proto >= \u001b[32m4\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1624\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_getattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msys\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodules\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m   1625\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1626\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(sys.modules[module], name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.5/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pickle.py:326\u001b[39m, in \u001b[36m_getattribute\u001b[39m\u001b[34m(obj, name)\u001b[39m\n\u001b[32m    324\u001b[39m         obj = \u001b[38;5;28mgetattr\u001b[39m(obj, subpath)\n\u001b[32m    325\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m326\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt get attribute \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    327\u001b[39m                              .format(name, top)) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, parent\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'TextPreprocessor' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('../data/combined_news_dataset.csv')\n",
    "\n",
    "# Load the preprocessor\n",
    "preprocessor = joblib.load('../models/text_preprocessor.pkl')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text if not already done\n",
    "if 'processed_text' not in df.columns:\n",
    "    print(\"Preprocessing text...\")\n",
    "    df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "    df['processed_text'] = df['combined_text'].apply(preprocessor.preprocess)\n",
    "    df = df[df['processed_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "# For CNN, we'll use less aggressive preprocessing (keep more structure)\n",
    "def light_preprocess(text):\n",
    "    \"\"\"Lighter preprocessing for CNN to preserve more text structure\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "import re\n",
    "df['cnn_text'] = (df['title'] + ' ' + df['text']).apply(light_preprocess)\n",
    "df = df[df['cnn_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization and Sequence Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Prepare the data\n",
    "texts = df['cnn_text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Found {len(word_index)} unique tokens\")\n",
    "print(f\"Vocabulary size (limited): {MAX_VOCAB_SIZE}\")\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "y = labels\n",
    "\n",
    "print(f\"Shape of data tensor: {X.shape}\")\n",
    "print(f\"Shape of label tensor: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sequence_lengths, bins=50, alpha=0.7)\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_SEQUENCE_LENGTH})')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(sequence_lengths, bins=50, alpha=0.7, cumulative=True, density=True)\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_SEQUENCE_LENGTH})')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Cumulative Proportion')\n",
    "plt.title('Cumulative Distribution of Sequence Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"Mean: {np.mean(sequence_lengths):.1f}\")\n",
    "print(f\"Median: {np.median(sequence_lengths):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(sequence_lengths, 95):.1f}\")\n",
    "print(f\"Percentage of sequences <= {MAX_SEQUENCE_LENGTH}: {(np.array(sequence_lengths) <= MAX_SEQUENCE_LENGTH).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nLabel distribution in training set: {np.bincount(y_train)}\")\n",
    "print(f\"Label distribution in validation set: {np.bincount(y_val)}\")\n",
    "print(f\"Label distribution in test set: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a CNN model for text classification\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        \n",
    "        # First CNN block\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second CNN block\n",
    "        Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third CNN block\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "cnn_model = create_cnn_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"CNN Model Architecture:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-filter CNN Model (Alternative Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a multi-filter CNN model inspired by Kim (2014)\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
    "    \n",
    "    # Multiple convolution filters with different kernel sizes\n",
    "    conv_blocks = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedding)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        conv_blocks.append(pool)\n",
    "    \n",
    "    # Concatenate all the pooled features\n",
    "    concatenated = Concatenate()(conv_blocks)\n",
    "    \n",
    "    # Dropout and dense layers\n",
    "    dropout1 = Dropout(0.5)(concatenated)\n",
    "    dense1 = Dense(128, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense1)\n",
    "    output = Dense(1, activation='sigmoid')(dropout2)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the multi-filter CNN model\n",
    "multi_cnn_model = create_multi_cnn_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile the model\n",
    "multi_cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nMulti-filter CNN Model Architecture:\")\n",
    "multi_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Standard CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Standard CNN Model...\")\n",
    "\n",
    "# Train the model\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi-filter CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Multi-filter CNN Model...\")\n",
    "\n",
    "# Train the multi-filter model\n",
    "history_multi_cnn = multi_cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title(f'{title} - Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title(f'{title} - Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(history_cnn, \"Standard CNN\")\n",
    "plot_training_history(history_multi_cnn, \"Multi-filter CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "print(\"Evaluating Standard CNN Model:\")\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {cnn_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating Multi-filter CNN Model:\")\n",
    "multi_cnn_loss, multi_cnn_accuracy = multi_cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {multi_cnn_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {multi_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "cnn_pred_proba = cnn_model.predict(X_test, verbose=0)\n",
    "cnn_pred = (cnn_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "multi_cnn_pred_proba = multi_cnn_model.predict(X_test, verbose=0)\n",
    "multi_cnn_pred = (multi_cnn_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification reports\n",
    "print(\"Standard CNN Classification Report:\")\n",
    "print(classification_report(y_test, cnn_pred, target_names=['Fake', 'True']))\n",
    "\n",
    "print(\"\\nMulti-filter CNN Classification Report:\")\n",
    "print(classification_report(y_test, multi_cnn_pred, target_names=['Fake', 'True']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Standard CNN confusion matrix\n",
    "cm_cnn = confusion_matrix(y_test, cnn_pred)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "           xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "ax1.set_title(f'Standard CNN\\nAccuracy: {cnn_accuracy:.3f}')\n",
    "ax1.set_ylabel('Actual Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# Multi-filter CNN confusion matrix\n",
    "cm_multi_cnn = confusion_matrix(y_test, multi_cnn_pred)\n",
    "sns.heatmap(cm_multi_cnn, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "           xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "ax2.set_title(f'Multi-filter CNN\\nAccuracy: {multi_cnn_accuracy:.3f}')\n",
    "ax2.set_ylabel('Actual Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Standard CNN', 'Multi-filter CNN'],\n",
    "    'Accuracy': [cnn_accuracy, multi_cnn_accuracy],\n",
    "    'Loss': [cnn_loss, multi_cnn_loss]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Accuracy', ax=ax1)\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylim(0.8, 1.0)  # Zoom in on the relevant range\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    ax1.text(i, v + 0.005, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Loss comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Loss', ax=ax2)\n",
    "ax2.set_title('Model Loss Comparison')\n",
    "for i, v in enumerate(comparison_df['Loss']):\n",
    "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model = multi_cnn_model if multi_cnn_accuracy > cnn_accuracy else cnn_model\n",
    "best_model_name = \"multi_cnn\" if multi_cnn_accuracy > cnn_accuracy else \"standard_cnn\"\n",
    "\n",
    "best_model.save(f'../models/cnn_{best_model_name}.h5')\n",
    "print(f\"Best CNN model saved: ../models/cnn_{best_model_name}.h5\")\n",
    "\n",
    "# Save both models\n",
    "cnn_model.save('../models/cnn_standard.h5')\n",
    "multi_cnn_model.save('../models/cnn_multi_filter.h5')\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open('../models/cnn_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save model configuration\n",
    "config = {\n",
    "    'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS\n",
    "}\n",
    "\n",
    "with open('../models/cnn_config.pickle', 'wb') as handle:\n",
    "    pickle.dump(config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"\\nAll CNN models and configurations saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new text\n",
    "def predict_fake_news(model, tokenizer, text, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"Predict if a text is fake news\"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = light_preprocess(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_prob = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    prediction = \"True News\" if prediction_prob > 0.5 else \"Fake News\"\n",
    "    \n",
    "    return prediction, prediction_prob\n",
    "\n",
    "# Test on some examples from the test set\n",
    "test_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Get original text\n",
    "    original_idx = df.index[df.index.isin(range(len(X_test)))][idx]\n",
    "    original_text = df.loc[original_idx, 'title'] + ' ' + df.loc[original_idx, 'text'][:200]\n",
    "    true_label = \"True News\" if y_test[idx] == 1 else \"Fake News\"\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction, prob = predict_fake_news(best_model, tokenizer, original_text)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {original_text[:200]}...\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {prediction} (Confidence: {prob:.3f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### CNN Model Results:\n",
    "- **Standard CNN**: Sequential architecture with multiple Conv1D layers\n",
    "- **Multi-filter CNN**: Parallel filters with different kernel sizes (inspired by Kim 2014)\n",
    "\n",
    "### Key Findings:\n",
    "1. CNNs can effectively capture local patterns in text for fake news detection\n",
    "2. Multi-filter architecture allows capturing different n-gram patterns simultaneously\n",
    "3. Global max pooling helps extract the most important features\n",
    "4. Dropout and early stopping help prevent overfitting\n",
    "\n",
    "### Advantages of CNN approach:\n",
    "- Faster training compared to RNNs\n",
    "- Good at capturing local patterns and n-grams\n",
    "- Parallel processing of different filter sizes\n",
    "- Less prone to vanishing gradient problems\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement LSTM/RNN models for comparison\n",
    "2. Experiment with pre-trained embeddings (Word2Vec, GloVe)\n",
    "3. Try attention mechanisms\n",
    "4. Implement transformer-based models (BERT)\n",
    "5. Ensemble different models for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fake_news_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}