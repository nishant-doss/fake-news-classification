{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Text Classification Model\n",
    "\n",
    "This notebook implements a Convolutional Neural Network for fake news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, Concatenate\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('../data/combined_news_dataset.csv')\n",
    "\n",
    "# Load the preprocessor\n",
    "preprocessor = joblib.load('../models/text_preprocessor.pkl')\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Label distribution:\\n{df['label'].value_counts()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess text if not already done\n",
    "if 'processed_text' not in df.columns:\n",
    "    print(\"Preprocessing text...\")\n",
    "    df['combined_text'] = df['title'] + ' ' + df['text']\n",
    "    df['processed_text'] = df['combined_text'].apply(preprocessor.preprocess)\n",
    "    df = df[df['processed_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "# For CNN, we'll use less aggressive preprocessing (keep more structure)\n",
    "def light_preprocess(text):\n",
    "    \"\"\"Lighter preprocessing for CNN to preserve more text structure\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    \n",
    "    # Basic cleaning\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    \n",
    "    return text\n",
    "\n",
    "import re\n",
    "df['cnn_text'] = (df['title'] + ' ' + df['text']).apply(light_preprocess)\n",
    "df = df[df['cnn_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Tokenization and Sequence Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization parameters\n",
    "MAX_VOCAB_SIZE = 20000\n",
    "MAX_SEQUENCE_LENGTH = 500\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "# Prepare the data\n",
    "texts = df['cnn_text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "# Create tokenizer\n",
    "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Convert texts to sequences\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "print(f\"Found {len(word_index)} unique tokens\")\n",
    "print(f\"Vocabulary size (limited): {MAX_VOCAB_SIZE}\")\n",
    "\n",
    "# Pad sequences\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "y = labels\n",
    "\n",
    "print(f\"Shape of data tensor: {X.shape}\")\n",
    "print(f\"Shape of label tensor: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in sequences]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(sequence_lengths, bins=50, alpha=0.7)\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_SEQUENCE_LENGTH})')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sequence Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(sequence_lengths, bins=50, alpha=0.7, cumulative=True, density=True)\n",
    "plt.axvline(x=MAX_SEQUENCE_LENGTH, color='r', linestyle='--', label=f'Max Length ({MAX_SEQUENCE_LENGTH})')\n",
    "plt.xlabel('Sequence Length')\n",
    "plt.ylabel('Cumulative Proportion')\n",
    "plt.title('Cumulative Distribution of Sequence Lengths')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistics\n",
    "print(f\"\\nSequence length statistics:\")\n",
    "print(f\"Mean: {np.mean(sequence_lengths):.1f}\")\n",
    "print(f\"Median: {np.median(sequence_lengths):.1f}\")\n",
    "print(f\"95th percentile: {np.percentile(sequence_lengths, 95):.1f}\")\n",
    "print(f\"Percentage of sequences <= {MAX_SEQUENCE_LENGTH}: {(np.array(sequence_lengths) <= MAX_SEQUENCE_LENGTH).mean():.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "\n",
    "print(f\"\\nLabel distribution in training set: {np.bincount(y_train)}\")\n",
    "print(f\"Label distribution in validation set: {np.bincount(y_val)}\")\n",
    "print(f\"Label distribution in test set: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a CNN model for text classification\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Embedding layer\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        \n",
    "        # First CNN block\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second CNN block\n",
    "        Conv1D(filters=128, kernel_size=4, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third CNN block\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "        GlobalMaxPooling1D(),\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "cnn_model = create_cnn_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile the model\n",
    "cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Display model architecture\n",
    "print(\"CNN Model Architecture:\")\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-filter CNN Model (Alternative Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a multi-filter CNN model inspired by Kim (2014)\"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    input_layer = Input(shape=(max_length,))\n",
    "    \n",
    "    # Embedding layer\n",
    "    embedding = Embedding(vocab_size, embedding_dim, input_length=max_length)(input_layer)\n",
    "    \n",
    "    # Multiple convolution filters with different kernel sizes\n",
    "    conv_blocks = []\n",
    "    filter_sizes = [3, 4, 5]\n",
    "    \n",
    "    for filter_size in filter_sizes:\n",
    "        conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedding)\n",
    "        pool = GlobalMaxPooling1D()(conv)\n",
    "        conv_blocks.append(pool)\n",
    "    \n",
    "    # Concatenate all the pooled features\n",
    "    concatenated = Concatenate()(conv_blocks)\n",
    "    \n",
    "    # Dropout and dense layers\n",
    "    dropout1 = Dropout(0.5)(concatenated)\n",
    "    dense1 = Dense(128, activation='relu')(dropout1)\n",
    "    dropout2 = Dropout(0.5)(dense1)\n",
    "    output = Dense(1, activation='sigmoid')(dropout2)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create the multi-filter CNN model\n",
    "multi_cnn_model = create_multi_cnn_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# Compile the model\n",
    "multi_cnn_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nMulti-filter CNN Model Architecture:\")\n",
    "multi_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=3,\n",
    "    restore_best_weights=True,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.2,\n",
    "    patience=2,\n",
    "    min_lr=0.0001,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Standard CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Standard CNN Model...\")\n",
    "\n",
    "# Train the model\n",
    "history_cnn = cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Multi-filter CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Multi-filter CNN Model...\")\n",
    "\n",
    "# Train the multi-filter model\n",
    "history_multi_cnn = multi_cnn_model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history, title):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    # Plot training & validation accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title(f'{title} - Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Plot training & validation loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title(f'{title} - Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot training histories\n",
    "plot_training_history(history_cnn, \"Standard CNN\")\n",
    "plot_training_history(history_multi_cnn, \"Multi-filter CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models on test set\n",
    "print(\"Evaluating Standard CNN Model:\")\n",
    "cnn_loss, cnn_accuracy = cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {cnn_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {cnn_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nEvaluating Multi-filter CNN Model:\")\n",
    "multi_cnn_loss, multi_cnn_accuracy = multi_cnn_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {multi_cnn_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {multi_cnn_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "cnn_pred_proba = cnn_model.predict(X_test, verbose=0)\n",
    "cnn_pred = (cnn_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "multi_cnn_pred_proba = multi_cnn_model.predict(X_test, verbose=0)\n",
    "multi_cnn_pred = (multi_cnn_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification reports\n",
    "print(\"Standard CNN Classification Report:\")\n",
    "print(classification_report(y_test, cnn_pred, target_names=['Fake', 'True']))\n",
    "\n",
    "print(\"\\nMulti-filter CNN Classification Report:\")\n",
    "print(classification_report(y_test, multi_cnn_pred, target_names=['Fake', 'True']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrices\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Standard CNN confusion matrix\n",
    "cm_cnn = confusion_matrix(y_test, cnn_pred)\n",
    "sns.heatmap(cm_cnn, annot=True, fmt='d', cmap='Blues', ax=ax1,\n",
    "           xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "ax1.set_title(f'Standard CNN\\nAccuracy: {cnn_accuracy:.3f}')\n",
    "ax1.set_ylabel('Actual Label')\n",
    "ax1.set_xlabel('Predicted Label')\n",
    "\n",
    "# Multi-filter CNN confusion matrix\n",
    "cm_multi_cnn = confusion_matrix(y_test, multi_cnn_pred)\n",
    "sns.heatmap(cm_multi_cnn, annot=True, fmt='d', cmap='Blues', ax=ax2,\n",
    "           xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "ax2.set_title(f'Multi-filter CNN\\nAccuracy: {multi_cnn_accuracy:.3f}')\n",
    "ax2.set_ylabel('Actual Label')\n",
    "ax2.set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Standard CNN', 'Multi-filter CNN'],\n",
    "    'Accuracy': [cnn_accuracy, multi_cnn_accuracy],\n",
    "    'Loss': [cnn_loss, multi_cnn_loss]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df)\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Accuracy comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Accuracy', ax=ax1)\n",
    "ax1.set_title('Model Accuracy Comparison')\n",
    "ax1.set_ylim(0.8, 1.0)  # Zoom in on the relevant range\n",
    "for i, v in enumerate(comparison_df['Accuracy']):\n",
    "    ax1.text(i, v + 0.005, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Loss comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Loss', ax=ax2)\n",
    "ax2.set_title('Model Loss Comparison')\n",
    "for i, v in enumerate(comparison_df['Loss']):\n",
    "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best performing model\n",
    "best_model = multi_cnn_model if multi_cnn_accuracy > cnn_accuracy else cnn_model\n",
    "best_model_name = \"multi_cnn\" if multi_cnn_accuracy > cnn_accuracy else \"standard_cnn\"\n",
    "\n",
    "best_model.save(f'../models/cnn_{best_model_name}.h5')\n",
    "print(f\"Best CNN model saved: ../models/cnn_{best_model_name}.h5\")\n",
    "\n",
    "# Save both models\n",
    "cnn_model.save('../models/cnn_standard.h5')\n",
    "multi_cnn_model.save('../models/cnn_multi_filter.h5')\n",
    "\n",
    "# Save tokenizer\n",
    "import pickle\n",
    "with open('../models/cnn_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# Save model configuration\n",
    "config = {\n",
    "    'max_vocab_size': MAX_VOCAB_SIZE,\n",
    "    'max_sequence_length': MAX_SEQUENCE_LENGTH,\n",
    "    'embedding_dim': EMBEDDING_DIM,\n",
    "    'batch_size': BATCH_SIZE,\n",
    "    'epochs': EPOCHS\n",
    "}\n",
    "\n",
    "with open('../models/cnn_config.pickle', 'wb') as handle:\n",
    "    pickle.dump(config, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "print(\"\\nAll CNN models and configurations saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new text\n",
    "def predict_fake_news(model, tokenizer, text, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"Predict if a text is fake news\"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = light_preprocess(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_prob = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    prediction = \"True News\" if prediction_prob > 0.5 else \"Fake News\"\n",
    "    \n",
    "    return prediction, prediction_prob\n",
    "\n",
    "# Test on some examples from the test set\n",
    "test_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "print(\"Sample Predictions:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Get original text\n",
    "    original_idx = df.index[df.index.isin(range(len(X_test)))][idx]\n",
    "    original_text = df.loc[original_idx, 'title'] + ' ' + df.loc[original_idx, 'text'][:200]\n",
    "    true_label = \"True News\" if y_test[idx] == 1 else \"Fake News\"\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction, prob = predict_fake_news(best_model, tokenizer, original_text)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {original_text[:200]}...\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {prediction} (Confidence: {prob:.3f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### CNN Model Results:\n",
    "- **Standard CNN**: Sequential architecture with multiple Conv1D layers\n",
    "- **Multi-filter CNN**: Parallel filters with different kernel sizes (inspired by Kim 2014)\n",
    "\n",
    "### Key Findings:\n",
    "1. CNNs can effectively capture local patterns in text for fake news detection\n",
    "2. Multi-filter architecture allows capturing different n-gram patterns simultaneously\n",
    "3. Global max pooling helps extract the most important features\n",
    "4. Dropout and early stopping help prevent overfitting\n",
    "\n",
    "### Advantages of CNN approach:\n",
    "- Faster training compared to RNNs\n",
    "- Good at capturing local patterns and n-grams\n",
    "- Parallel processing of different filter sizes\n",
    "- Less prone to vanishing gradient problems\n",
    "\n",
    "### Next Steps:\n",
    "1. Implement LSTM/RNN models for comparison\n",
    "2. Experiment with pre-trained embeddings (Word2Vec, GloVe)\n",
    "3. Try attention mechanisms\n",
    "4. Implement transformer-based models (BERT)\n",
    "5. Ensemble different models for better performance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}