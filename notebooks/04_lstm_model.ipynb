{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM/RNN Model for Fake News Classification\n",
    "\n",
    "This notebook implements Long Short-Term Memory (LSTM) and other RNN architectures for fake news classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, GRU, SimpleRNN, Bidirectional\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras.layers import SpatialDropout1D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/combined_news_dataset.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "\n",
    "# Load the tokenizer from CNN notebook (or create if not available)\n",
    "try:\n",
    "    with open('../models/cnn_tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "    \n",
    "    with open('../models/cnn_config.pickle', 'rb') as handle:\n",
    "        config = pickle.load(handle)\n",
    "    \n",
    "    MAX_VOCAB_SIZE = config['max_vocab_size']\n",
    "    MAX_SEQUENCE_LENGTH = config['max_sequence_length']\n",
    "    EMBEDDING_DIM = config['embedding_dim']\n",
    "    \n",
    "    print(\"Loaded existing tokenizer and configuration\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Creating new tokenizer...\")\n",
    "    \n",
    "    # Configuration\n",
    "    MAX_VOCAB_SIZE = 20000\n",
    "    MAX_SEQUENCE_LENGTH = 500\n",
    "    EMBEDDING_DIM = 100\n",
    "    \n",
    "    # Light preprocessing function\n",
    "    import re\n",
    "    def light_preprocess(text):\n",
    "        if pd.isna(text):\n",
    "            return ''\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "        text = re.sub(r'\\S+@\\S+', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = ' '.join(text.split())\n",
    "        return text\n",
    "    \n",
    "    # Prepare text\n",
    "    df['cnn_text'] = (df['title'] + ' ' + df['text']).apply(light_preprocess)\n",
    "    df = df[df['cnn_text'] != ''].reset_index(drop=True)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE, oov_token='<OOV>')\n",
    "    tokenizer.fit_on_texts(df['cnn_text'].values)\n",
    "\n",
    "print(f\"Vocabulary size: {MAX_VOCAB_SIZE}\")\n",
    "print(f\"Max sequence length: {MAX_SEQUENCE_LENGTH}\")\n",
    "print(f\"Embedding dimension: {EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Light preprocessing function (if not already defined)\n",
    "import re\n",
    "def light_preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n",
    "\n",
    "# Prepare the text data\n",
    "if 'cnn_text' not in df.columns:\n",
    "    df['cnn_text'] = (df['title'] + ' ' + df['text']).apply(light_preprocess)\n",
    "    df = df[df['cnn_text'] != ''].reset_index(drop=True)\n",
    "\n",
    "# Convert texts to sequences\n",
    "texts = df['cnn_text'].values\n",
    "labels = df['label'].values\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH, padding='post', truncating='post')\n",
    "y = labels\n",
    "\n",
    "print(f\"Data shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Label distribution: {np.bincount(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Validation set shape: {X_val.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Simple LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a simple LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "simple_lstm = create_simple_lstm_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "simple_lstm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Simple LSTM Model:\")\n",
    "simple_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Bidirectional LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bidirectional_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a bidirectional LSTM model\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(LSTM(100, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "bidirectional_lstm = create_bidirectional_lstm_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "bidirectional_lstm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nBidirectional LSTM Model:\")\n",
    "bidirectional_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Stacked LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stacked_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a stacked LSTM model with multiple LSTM layers\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        \n",
    "        # First LSTM layer\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2, return_sequences=True),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "stacked_lstm = create_stacked_lstm_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "stacked_lstm.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nStacked LSTM Model:\")\n",
    "stacked_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. GRU Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gru_model(vocab_size, embedding_dim, max_length):\n",
    "    \"\"\"Create a GRU model (faster alternative to LSTM)\"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "        SpatialDropout1D(0.2),\n",
    "        Bidirectional(GRU(100, dropout=0.2, recurrent_dropout=0.2)),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create and compile the model\n",
    "gru_model = create_gru_model(MAX_VOCAB_SIZE, EMBEDDING_DIM, MAX_SEQUENCE_LENGTH)\n",
    "gru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"\\nGRU Model:\")\n",
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Callbacks\n",
    "def get_callbacks(model_name):\n",
    "    return [\n",
    "        EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.2,\n",
    "            patience=2,\n",
    "            min_lr=0.0001,\n",
    "            verbose=1\n",
    "        ),\n",
    "        ModelCheckpoint(\n",
    "            f'../models/best_{model_name}.h5',\n",
    "            monitor='val_accuracy',\n",
    "            save_best_only=True,\n",
    "            verbose=1\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store models and their histories\n",
    "models = {\n",
    "    'Simple LSTM': simple_lstm,\n",
    "    'Bidirectional LSTM': bidirectional_lstm,\n",
    "    'Stacked LSTM': stacked_lstm,\n",
    "    'GRU': gru_model\n",
    "}\n",
    "\n",
    "histories = {}\n",
    "results = {}\n",
    "\n",
    "# Train each model\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get callbacks for this model\n",
    "    callbacks = get_callbacks(name.lower().replace(' ', '_'))\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS,\n",
    "        validation_data=(X_val, y_val),\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    histories[name] = history\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    results[name] = {'loss': test_loss, 'accuracy': test_accuracy}\n",
    "    \n",
    "    print(f\"Test Accuracy for {name}: {test_accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training histories for all models\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, (name, history) in enumerate(histories.items()):\n",
    "    # Accuracy plot\n",
    "    ax1 = axes[idx]\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title(f'{name} - Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Loss plot\n",
    "    ax2 = axes[idx + 4]\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title(f'{name} - Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': list(results.keys()),\n",
    "    'Test_Accuracy': [results[model]['accuracy'] for model in results.keys()],\n",
    "    'Test_Loss': [results[model]['loss'] for model in results.keys()]\n",
    "})\n",
    "\n",
    "comparison_df = comparison_df.sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Plot comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Accuracy comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Test_Accuracy', ax=ax1)\n",
    "ax1.set_title('Test Accuracy Comparison')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.set_ylim(0.8, 1.0)\n",
    "\n",
    "for i, v in enumerate(comparison_df['Test_Accuracy']):\n",
    "    ax1.text(i, v + 0.005, f'{v:.3f}', ha='center')\n",
    "\n",
    "# Loss comparison\n",
    "sns.barplot(data=comparison_df, x='Model', y='Test_Loss', ax=ax2)\n",
    "ax2.set_title('Test Loss Comparison')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "for i, v in enumerate(comparison_df['Test_Loss']):\n",
    "    ax2.text(i, v + 0.01, f'{v:.3f}', ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best model\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "print(f\"Best performing model: {best_model_name}\")\n",
    "print(f\"Test Accuracy: {comparison_df.iloc[0]['Test_Accuracy']:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred_proba = best_model.predict(X_test, verbose=0)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "\n",
    "# Classification report\n",
    "print(f\"\\nClassification Report for {best_model_name}:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['Fake', 'True']))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "           xticklabels=['Fake', 'True'], yticklabels=['Fake', 'True'])\n",
    "plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n",
    "\n",
    "# Additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f\"\\nDetailed Metrics:\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1_score:.4f}\")\n",
    "print(f\"True Positives: {tp}, False Positives: {fp}\")\n",
    "print(f\"True Negatives: {tn}, False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Confidence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze prediction confidence\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Histogram of prediction probabilities\n",
    "ax1.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.7, label='Fake News', density=True)\n",
    "ax1.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.7, label='True News', density=True)\n",
    "ax1.axvline(x=0.5, color='red', linestyle='--', label='Decision Threshold')\n",
    "ax1.set_xlabel('Predicted Probability')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Distribution of Prediction Probabilities')\n",
    "ax1.legend()\n",
    "\n",
    "# Confidence vs accuracy\n",
    "confidence = np.maximum(y_pred_proba.flatten(), 1 - y_pred_proba.flatten())\n",
    "correct = (y_pred == y_test).astype(int)\n",
    "\n",
    "# Bin by confidence and calculate accuracy\n",
    "n_bins = 10\n",
    "bin_boundaries = np.linspace(0.5, 1.0, n_bins + 1)\n",
    "bin_lowers = bin_boundaries[:-1]\n",
    "bin_uppers = bin_boundaries[1:]\n",
    "\n",
    "accuracies = []\n",
    "confidences = []\n",
    "counts = []\n",
    "\n",
    "for bin_lower, bin_upper in zip(bin_lowers, bin_uppers):\n",
    "    in_bin = (confidence > bin_lower) & (confidence <= bin_upper)\n",
    "    if in_bin.sum() > 0:\n",
    "        accuracy_in_bin = correct[in_bin].mean()\n",
    "        avg_confidence_in_bin = confidence[in_bin].mean()\n",
    "        count_in_bin = in_bin.sum()\n",
    "        \n",
    "        accuracies.append(accuracy_in_bin)\n",
    "        confidences.append(avg_confidence_in_bin)\n",
    "        counts.append(count_in_bin)\n",
    "\n",
    "ax2.bar(range(len(accuracies)), accuracies, alpha=0.7, label='Accuracy')\n",
    "ax2.plot(range(len(confidences)), confidences, 'ro-', label='Confidence')\n",
    "ax2.plot([0, len(accuracies)-1], [0, 1], 'k--', alpha=0.5, label='Perfect Calibration')\n",
    "ax2.set_xlabel('Confidence Bin')\n",
    "ax2.set_ylabel('Accuracy / Confidence')\n",
    "ax2.set_title('Model Calibration')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nCalibration Analysis:\")\n",
    "for i, (acc, conf, count) in enumerate(zip(accuracies, confidences, counts)):\n",
    "    print(f\"Bin {i+1}: Confidence={conf:.3f}, Accuracy={acc:.3f}, Count={count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions on new text\n",
    "def predict_with_lstm(model, tokenizer, text, max_length=MAX_SEQUENCE_LENGTH):\n",
    "    \"\"\"Predict if a text is fake news using LSTM model\"\"\"\n",
    "    # Preprocess the text\n",
    "    processed_text = light_preprocess(text)\n",
    "    \n",
    "    # Convert to sequence\n",
    "    sequence = tokenizer.texts_to_sequences([processed_text])\n",
    "    padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post', truncating='post')\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction_prob = model.predict(padded_sequence, verbose=0)[0][0]\n",
    "    prediction = \"True News\" if prediction_prob > 0.5 else \"Fake News\"\n",
    "    \n",
    "    return prediction, prediction_prob\n",
    "\n",
    "# Test on some examples\n",
    "test_indices = np.random.choice(len(X_test), 5, replace=False)\n",
    "\n",
    "print(f\"Sample Predictions using {best_model_name}:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, idx in enumerate(test_indices):\n",
    "    # Get original text (this is a simplified approach)\n",
    "    original_text = df.iloc[idx]['title'] + ' ' + df.iloc[idx]['text'][:300]\n",
    "    true_label = \"True News\" if y_test[idx] == 1 else \"Fake News\"\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction, prob = predict_with_lstm(best_model, tokenizer, original_text)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Text: {original_text[:250]}...\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {prediction} (Confidence: {prob:.3f})\")\n",
    "    status = \"✓\" if prediction == true_label else \"✗\"\n",
    "    print(f\"Correct: {status}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "best_model.save(f'../models/lstm_best_model.h5')\n",
    "\n",
    "# Save all models\n",
    "for name, model in models.items():\n",
    "    safe_name = name.lower().replace(' ', '_')\n",
    "    model.save(f'../models/lstm_{safe_name}.h5')\n",
    "\n",
    "# Save the results\n",
    "results_df = pd.DataFrame([\n",
    "    {'model': name, 'accuracy': results[name]['accuracy'], 'loss': results[name]['loss']} \n",
    "    for name in results.keys()\n",
    "])\n",
    "results_df.to_csv('../results/lstm_model_comparison.csv', index=False)\n",
    "\n",
    "print(f\"\\nBest model ({best_model_name}) saved as: ../models/lstm_best_model.h5\")\n",
    "print(\"All models and results saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Time Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table with model complexity\n",
    "model_params = {}\n",
    "for name, model in models.items():\n",
    "    model_params[name] = model.count_params()\n",
    "\n",
    "summary_df = pd.DataFrame({\n",
    "    'Model': list(model_params.keys()),\n",
    "    'Parameters': list(model_params.values()),\n",
    "    'Test_Accuracy': [results[name]['accuracy'] for name in model_params.keys()],\n",
    "    'Test_Loss': [results[name]['loss'] for name in model_params.keys()]\n",
    "})\n",
    "\n",
    "summary_df = summary_df.sort_values('Test_Accuracy', ascending=False)\n",
    "summary_df['Parameters_M'] = summary_df['Parameters'] / 1e6\n",
    "\n",
    "print(\"Model Complexity vs Performance:\")\n",
    "print(summary_df[['Model', 'Parameters_M', 'Test_Accuracy', 'Test_Loss']].to_string(index=False))\n",
    "\n",
    "# Plot parameters vs accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=summary_df, x='Parameters_M', y='Test_Accuracy', s=100)\n",
    "\n",
    "for i, row in summary_df.iterrows():\n",
    "    plt.annotate(row['Model'], (row['Parameters_M'], row['Test_Accuracy']), \n",
    "                xytext=(5, 5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Model Parameters (Millions)')\n",
    "plt.ylabel('Test Accuracy')\n",
    "plt.title('Model Complexity vs Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### RNN/LSTM Model Results:\n",
    "- **Simple LSTM**: Basic LSTM architecture\n",
    "- **Bidirectional LSTM**: Processes sequences in both directions\n",
    "- **Stacked LSTM**: Multiple LSTM layers for deeper feature extraction\n",
    "- **GRU**: Faster alternative to LSTM with similar performance\n",
    "\n",
    "### Key Findings:\n",
    "1. LSTM models can capture sequential patterns and long-range dependencies in text\n",
    "2. Bidirectional processing often improves performance by considering future context\n",
    "3. Stacked architectures may provide deeper understanding but risk overfitting\n",
    "4. GRU models are faster to train while maintaining competitive performance\n",
    "5. Proper regularization (dropout, early stopping) is crucial for RNNs\n",
    "\n",
    "### Advantages of LSTM approach:\n",
    "- Excellent at capturing sequential dependencies\n",
    "- Can handle variable-length inputs naturally\n",
    "- Bidirectional processing captures full context\n",
    "- Good performance on text classification tasks\n",
    "\n",
    "### Disadvantages:\n",
    "- Slower training compared to CNNs\n",
    "- Sequential processing prevents parallelization\n",
    "- Prone to overfitting on smaller datasets\n",
    "- Memory intensive for long sequences\n",
    "\n",
    "### Next Steps:\n",
    "1. Experiment with attention mechanisms\n",
    "2. Try pre-trained embeddings (Word2Vec, GloVe, FastText)\n",
    "3. Implement transformer-based models (BERT, RoBERTa)\n",
    "4. Ensemble multiple models for improved performance\n",
    "5. Fine-tune hyperparameters and architecture details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}